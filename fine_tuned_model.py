from tqdm import tqdm
from transformers import CLIPTokenizer, CLIPTextModel
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader, Dataset
import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd

class PlotDataset(Dataset):
    """
    Represents a custom dataset used for text embedding with SentenceTransformer.
    This class is designed to handle a collection of texts, encoding them into
    embeddings, and making them accessible in a dataset format compatible with
    PyTorch utilities. It leverages the SentenceTransformer model with the
    'all-MiniLM-L6-v2' pre-trained model to generate embeddings for the
    provided texts.
    :ivar texts: A collection of text strings that the dataset operates upon.
    :type texts: list
    :ivar sbert: An instance of the SentenceTransformer model used for text embedding.
    :type sbert: SentenceTransformer
    :ivar targets: A tensor containing the pre-computed embeddings for the `texts`.
                   Generated using the SentenceTransformer model during initialization.
    :type targets: torch.Tensor
    """
    def __init__(self, texts):
        self.texts = texts
        self.sbert = SentenceTransformer('all-MiniLM-L6-v2')
        with torch.no_grad():
            self.targets = self.sbert.encode(texts, convert_to_tensor=True, normalize_embeddings=True)
            self.targets = self.targets.to('cuda' if torch.cuda.is_available() else 'cpu')

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx], self.targets[idx]

# Model setup
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch32")
clip_text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-base-patch32")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Add a linear layer to project embeddings to the same dimension
projection_layer = nn.Linear(512, 384)
clip_text_encoder.to(device)
projection_layer.to(device)

def train_clip_on_sbert(texts, epochs=10, lr=1e-5, batch_size=16, save_path="clip-finetuned-sbert"):
    """
    Trains a CLIP-style text encoder model on embeddings generated by SBERT,
    fine-tuning the CLIP text encoder while projecting its embeddings into a
    space matching that of the SBERT outputs. This function also normalizes the
    embeddings and calculates the cosine similarity loss to align both representations.
    :param texts: The list of text data to train on.
    :param epochs: The number of training epochs. Default is 10.
    :type epochs: int
    :param lr: The learning rate for the optimizer. Default is 1e-5.
    :type lr: float
    :param batch_size: The batch size for the DataLoader. Default is 16.
    :type batch_size: int
    :param save_path: The path where the fine-tuned model and tokenizer will be saved.
        Default is "clip-finetuned-sbert".
    :type save_path: str
    :return: The fine-tuned CLIP text encoder model.
    :rtype: Any
    """
    dataset = PlotDataset(texts)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Combine both the text encoder and linear projection layer into training
    optimizer = optim.AdamW(list(clip_text_encoder.parameters()) + list(projection_layer.parameters()), lr=lr)
    loss_fn = nn.CosineEmbeddingLoss()

    clip_text_encoder.train()
    projection_layer.train()

    for epoch in range(epochs):
        total_loss = 0
        for batch_texts, batch_targets in tqdm(dataloader, desc=f"Training Epoch"):
            # Tokenize input
            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors="pt")
            inputs.to(device)

            # Forward pass through CLIP text encoder
            outputs = clip_text_encoder(**inputs).last_hidden_state[:, 0, :]  # CLS token

            # Normalize outputs
            outputs = nn.functional.normalize(outputs, p=2, dim=1)

            # Project CLIP embeddings to 384 dimensions
            outputs = projection_layer(outputs)

            # Normalize SBERT targets
            batch_targets = nn.functional.normalize(batch_targets, p=2, dim=1)

            # Create label vector of +1 (we want similarity)
            targets = torch.ones(outputs.size(0), device=device)

            # Move tensors to the same device
            device = outputs.device
            batch_targets = batch_targets.to(device)
            targets = targets.to(device)

            # Compute loss
            loss = loss_fn(outputs, batch_targets, targets)

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} | Average Loss: {avg_loss:.4f}")

    # Save model and tokenizer
    clip_text_encoder.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)

    return clip_text_encoder

def preprocess_data(csv_path="IMBD DATASET.csv"):
    """
    Reads a CSV file, processes the data by removing specific columns, duplicates, and unwanted rows,
    and converts boolean columns to integer type. The function returns the processed dataframe.
    :param csv_path: The file path to the CSV file to be processed.
    :type csv_path: str, optional
    :return: A pandas DataFrame object containing the processed data.
    :rtype: pandas.DataFrame
    """
    df = pd.read_csv(csv_path)
    df.drop(df.columns[0], axis=1, inplace=True)
    cols_to_remove = ["url", "year", "certificate", "rating", "votes"]
    df.drop(columns=cols_to_remove, errors="ignore" ,inplace=True)
    df.drop_duplicates(keep="first", inplace=True)
    df = df[~df["plot"].str.contains("Add a Plot", na=False)]
    bool_cols = df.select_dtypes(include="bool").columns
    df[bool_cols] = df[bool_cols].astype(int)
    return df


if __name__ == '__main__':
    csv_path = "IMBD DATASET.csv"
    data = preprocess_data(csv_path)

    data['text'] = 'Title: ' + data['name'].fillna('') + '. Plot: ' + data['plot'].fillna('')
    texts = data['text'].tolist()
    # Train the CLIP model on SBERT embeddings
    trained_model = train_clip_on_sbert(texts, epochs=15, lr=1e-5, batch_size=16, save_path="clip-finetuned-sbert")

